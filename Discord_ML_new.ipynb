{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f39a60d5",
   "metadata": {},
   "source": [
    "## Step 1: Creating the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acebcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'text': [\n",
    "        \"\"\"OldChunkCoal has asked me to write six paragraphs anywhere from 2 to 4 sentences long. Who does he think he is? He's asking me to write a full six paragraph essay that changes subject every paragraph. Crazy.\"\"\",\n",
    "        \"\"\"I used to have a YouTube channel back in 2011. Back then you could customize the color and background of your channel page. My channel was all about Minecraft adventure maps. The main color was black with red highlights and the background was the Minecraft sand texture.\"\"\",\n",
    "        \"\"\"I recently read a book called \"How to Stop Time\" by Matt Haig. I recommend it. It is about a man who ages approximately 15 times slower then normal. He was born in the 1500's and lives over 900 years. The story heavily focuses on grief as someone who lives longer than everyone they will ever know has to deal with a lot of death.\"\"\",\n",
    "        \"\"\"Have you ever baked bread before? It's pretty easy. At the very least, all you need is flour, water, yeast, and a few hours.\"\"\",\n",
    "        \"\"\"AI is awesome and scary. It has the potential to help us create amazing new things, but I worry about the damage it may allow some to do. There is also the sci-fi horror of a rouge AI.\"\"\",\n",
    "        \"\"\"Loki is a Disney+ tv show about a variant of the Marvel character Loki. In season 1 he is kidnapped by agents of the Time Variance Authority who make it their responsibility to maintain the Sacred Timeline and to stop any variants from damaging the multiverse. It's a fun show. Highly recommend it.\"\"\",\n",
    "        \"\"\"The majestic blahaj is a crucial implement in any aspiring femboy’s arsenal. Naysayers may claim that “it’s just a plushie” or that “it doesn’t actually have feelings, it’s not real”. However, this has been proven decidedly false by leading researchers at Sigma University.\"\"\",\n",
    "        \"\"\"I love listening to the “basic American” accent, it’s like music to my ears. One time in an airport, while I was waiting in line for immigration control, two American guys were having a very loud conversation. I just got to bask in their mundane discussion about someone’s aunt. 10/10 experience\"\"\",\n",
    "        \"\"\"Did you know that one third of all food produced is actually wasted? This statistic is a bit more complicated than it seems though, since a lot of this food waste happens during the manufacturing process. But still, retail and end-user waste is a very significant contributing factor.\"\"\",\n",
    "        \"\"\"I fucking hate the Jordan Peterson lobster thing. I stay up at night thinking about it, clutching my fists, sweating, trying not to scream out “THEY’RE INVERTEBRATES” at the top of my lungs at 1 am. It’s haunting me.\"\"\",\n",
    "        \"\"\"I haven’t been getting much sleep recently. Yesterday I only got 4 hours of sleep, and today I got about 6. Not the worst result, but I am looking forward to finally laying down and getting some good quality shuteye tonight. As long as I don’t remember the fucking lobsters.\"\"\",\n",
    "        \"\"\"One example of cannibalism becoming a vector for the spread of disease is the history of the Fore people of Papua New Guinea. There, the consumption of human meat (spurred by the cultural practice of endocannibalism) caused a surge of Kuru, which is a type of Transmissible Spongiform Encephalopathy. This practice led to an epidemic which killed 2,700 people, and is likely to have been caused by a single affected individual in the 1900s.\"\"\",\n",
    "        \"\"\"This oldchunkcoal geezer asked me to write some paragraphs about some waffle. I used to play corejourney with him. He dies a lot on the server but I'd help him every time he died! He was on my team called Hobbits along with nootbot.\"\"\",\n",
    "        \"\"\"I love playing video games because i can connect with my friends and have fun! I enjoy playing minecraft, factorio, (rule 15)  and valorant!\"\"\",\n",
    "        \"\"\"I am currently studying my ass off for my final exams! It can be really stressful at times but i don't care. \"\"\",\n",
    "        \"\"\"pint of stella with lads its comin home, just beaten the wife for not making bangers and mash and she made some vegetables and tried to make me brush me teef i aint putting no white stuff in my mouth\"\"\",\n",
    "        \"\"\"Yes it’s very interesting, I despise people chewing gum in public, it’s illegal in Singapore. Soon we will be able to use ai to catch people breaking the law and ticket them for such things. It’s just a ticket, but when people chew gum it leads to other things like marijuana and eventually worse drugs, we have to catch them early, before it is too late!\"\"\",\n",
    "        \"\"\"copying other members announcements is NOT tolerated in this safe and loving environment, therefore I will have to remove your post from the channel as ‘plagiarism’ is a ‘dick move’\"\"\",\n",
    "        \n",
    "#         \"\"\"I love traveling and exploring beautiful new countries especially countries with beautiful national parks like the US and New Zealand and more! I hope to be able to travel to many more countries as I earn more money!\"\"\",\n",
    "#         \"\"\"I love weeb stuff and anything cutesy and somewhat not \"manly\". People can judge me but I simply do not care as long as I love the cute plushies. A blahaj is not femboy plushie and it is a fun plushie and a nice deco!!!\"\"\",\n",
    "#         \"\"\"Posado has been a closet furry for X years and refuses to admit to being one. But as an open-minded individual, I have learned to embrace weird things and respect people for their beliefs and hobbies alike.\"\"\",\n",
    "#         \"\"\"You may ask what is a floppa. A floppa is a type of wild cat from the African continent. it may be big, cuddly, and cute. But do not mistake it for a domestic cat. Always remember it is a wild animal at heart.\"\"\",\n",
    "#         \"\"\"I'm currently a student from a tiny island country. but I hope to know more people from around the world and the different cultures. As someone wise once said knowledge is power :< Do not qoute me on this shit smh...\"\"\",\n",
    "#         \"\"\"Oldchunkie is a big MC noob but too kind-hearted to being playing anarchy servers lol. But we all love and respect him for his uniqueness and playstyle. I hope this random essay helps his project.\"\"\",\n",
    "    ],\n",
    "    'author': [1,1,1,1,1,1,2,2,2,2,2,2,3,3,3,3,3,3]\n",
    "}\n",
    "\n",
    "# Author1 is dark\n",
    "# Author2 is Posado\n",
    "# Author3 is max\n",
    "\n",
    "# Author4 is noot (noots has been temporarily put on hold)\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85220007",
   "metadata": {},
   "source": [
    "## Step 2: Adding Features to the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159a58af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "nltk.download('words')\n",
    "modal_verbs = ['can', 'could', 'may', 'might', 'shall', 'should', 'will', 'would', 'must']\n",
    "\n",
    "\n",
    "# Function to count punctuation\n",
    "def count_punctuation(text):\n",
    "    count = 0\n",
    "    for char in text:\n",
    "        if char in string.punctuation:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "# Function to count capital letters\n",
    "def count_capitals(text):\n",
    "    count = 0\n",
    "    for char in text:\n",
    "        if char.isupper():\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "# Function to count 4 letter words\n",
    "def count_four_letter_words(text):\n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "    # Count the words with exactly four letters\n",
    "    four_letter_words_count = sum(len(word) == 4 for word in words)\n",
    "    return four_letter_words_count\n",
    "\n",
    "word_list = set(words.words())\n",
    "def percentage_dictionary_words(text):\n",
    "    words_in_text = text.split()  # Split the text into individual words\n",
    "    number_of_words = len(words_in_text)  # Count the total number of words\n",
    "    \n",
    "    # Initialize a counter for dictionary words\n",
    "    number_of_dictionary_words = 0\n",
    "    \n",
    "    # Iterate through each word in the text\n",
    "    for word in words_in_text:\n",
    "        # Remove punctuation and convert to lowercase\n",
    "        cleaned_word = word.lower().strip(string.punctuation)\n",
    "        # Check if the cleaned word is in the dictionary\n",
    "        if cleaned_word in word_list:\n",
    "            # If it is, increment the counter\n",
    "            number_of_dictionary_words += 1\n",
    "    \n",
    "    # Calculate the percentage of dictionary words\n",
    "    percentage = (number_of_dictionary_words / number_of_words) * 100 if number_of_words > 0 else 0\n",
    "    return percentage\n",
    "\n",
    "def count_modals_explicit(text):\n",
    "    # Normalize the text to lowercase and split into words\n",
    "    words = text.lower().split()\n",
    "    # Initialize a counter for modal verbs\n",
    "    modal_count = 0\n",
    "    # Iterate over each word in the text\n",
    "    for word in words:\n",
    "        # Check if the word is in the list of modal verbs\n",
    "        if word in modal_verbs:\n",
    "            # Increment the counter if it is a modal verb\n",
    "            modal_count += 1\n",
    "    # Return the total count of modal verbs found in the text\n",
    "    return modal_count\n",
    "\n",
    "\n",
    "\n",
    "# Apply functions to the 'text' column to create new columns for each feature\n",
    "df['punctuation_count'] = df['text'].apply(count_punctuation)\n",
    "df['capital_count'] = df['text'].apply(count_capitals)\n",
    "df['four_letter_word_count'] = df['text'].apply(count_four_letter_words)\n",
    "df['perc_dictionary_words'] = df['text'].apply(percentage_dictionary_words)\n",
    "df['modal_count'] = df['text'].apply(count_modals_explicit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f0c6f9",
   "metadata": {},
   "source": [
    "## Step 3: Converting DataFrame to NumPy Arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d0bf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Converting features and target variable to NumPy arrays\n",
    "X = df[['punctuation_count', 'capital_count', 'four_letter_word_count', 'perc_dictionary_words', 'modal_count']].to_numpy()  # Feature matrix\n",
    "y = df['author'].to_numpy()  # Target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2995e224",
   "metadata": {},
   "source": [
    "## EXTRA: Clustering Unsupervised Learning for insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88e9d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import KMeans\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# # Assuming X is your feature matrix from the DataFrame\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# # Choose the number of clusters\n",
    "# kmeans = KMeans(n_clusters=3, random_state=0)  # You might want to experiment with the number of clusters\n",
    "# clusters = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# # Add cluster information back to the DataFrame\n",
    "# df['cluster'] = clusters\n",
    "\n",
    "# # Now you can analyze the clusters\n",
    "# for i in range(3):\n",
    "#     cluster_data = df[df['cluster'] == i]\n",
    "#     print(f\"Cluster {i}:\")\n",
    "#     print(cluster_data.mean())  # Print the mean of features for each cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe6b171",
   "metadata": {},
   "source": [
    "## EXTRA: Test for overall accuracy depending on Test_Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3692e0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "test_sizes = [0.2, 0.3, 0.4]\n",
    "for test_size in test_sizes:\n",
    "    accuracies = []  # To store accuracy of each run\n",
    "    for _ in range(20):  # Number of runs for averaging\n",
    "        # Splitting the data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "        \n",
    "        # Scaling\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # Training the KNN model\n",
    "        knn = KNeighborsClassifier(n_neighbors=3)\n",
    "        knn.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Making predictions and evaluating\n",
    "        y_pred = knn.predict(X_test_scaled)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "    \n",
    "    # Averaging the accuracies\n",
    "    avg_accuracy = np.mean(accuracies)\n",
    "    print(f\"Average accuracy for test size {test_size}: {avg_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46778f1a",
   "metadata": {},
   "source": [
    "## Step 4: Splitting the Data & Scaling X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "777acd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# First, split your dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "\n",
    "# Then, initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to your training features and transform them\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the test set features with the fitted scaler\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize the KNN classifier with an example of k neighbors\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Train the KNN model\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = knn.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model's accuracy on the test set\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"KNN Model Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea98588b",
   "metadata": {},
   "source": [
    "## 6. Run Single Test (Optional Fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56242a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = \"Enter Text Here of Person to see which one is predicted\"\n",
    "# Assuming the count_punctuation and count_capitals functions are already defined\n",
    "\n",
    "new_punctuation_count = count_punctuation(new_text)\n",
    "new_capital_count = count_capitals(new_text)\n",
    "new_four_letter_word_count = count_four_letter_words(new_text)\n",
    "new_perc_dictionary_words = percentage_dictionary_words(new_text)\n",
    "new_modal_count = count_modals_explicit(new_text)\n",
    "\n",
    "# Option 2: Using NumPy (if you prefer working with arrays)\n",
    "import numpy as np\n",
    "new_features = np.array([[new_punctuation_count, new_capital_count, new_four_letter_word_count, new_perc_dictionary_words, new_modal_count]])\n",
    "\n",
    "new_features_scaled = scaler.transform(new_features)  # This line is necessary to scale the new features\n",
    "predicted_author_style = knn.predict(new_features_scaled)  # Make sure to use the scaled features here\n",
    "\n",
    "print(f\"The predicted author style is: {predicted_author_style[0]}\")\n",
    "\n",
    "# print(new_punctuation_count)\n",
    "# print(new_capital_count)\n",
    "# print(new_four_letter_word_count)\n",
    "# print(new_perc_dictionary_words)\n",
    "# print(new_modal_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea04e127",
   "metadata": {},
   "source": [
    "## 7. 3d Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b535d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Create a new figure for 3D plotting\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Different markers for different authors\n",
    "markers = ['o', 's', 'D']\n",
    "# Different colors for different authors\n",
    "colors = ['red', 'green', 'blue']\n",
    "\n",
    "# print(y)\n",
    "# print(y_train)\n",
    "# print(y_test)\n",
    "\n",
    "# Plot each author's data points in the 3D space\n",
    "for i, author in enumerate(np.unique(y_train)):\n",
    "    idx = np.where(y_train == author)\n",
    "#     print(y_train)\n",
    "#     print(author)\n",
    "#     print(idx)\n",
    "    ax.scatter(X_train_scaled[idx, 0], X_train_scaled[idx, 1], X_train_scaled[idx, 2], label=f'Author {author}', \n",
    "               marker=markers[i], color=colors[i], s=100)\n",
    "    \n",
    "\n",
    "# Set labels for the axes\n",
    "ax.set_xlabel('Punctuation Count')\n",
    "ax.set_ylabel('Capital Letter Count')\n",
    "ax.set_zlabel('Four Letter Word Count')\n",
    "\n",
    "# Title and legend\n",
    "plt.title('Feature Comparison by Author')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5ce931",
   "metadata": {},
   "source": [
    "## Extra: 4d Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35506ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Assuming df is your DataFrame and it has been split into training and testing sets\n",
    "\n",
    "# Normalizing size based on 'perc_dictionary_words' or another feature for the 4th dimension\n",
    "size_feature = df['perc_dictionary_words'].values  # Ensure this line is before splitting if not already\n",
    "size_norm = np.interp(size_feature, (size_feature.min(), size_feature.max()), (50, 200))\n",
    "\n",
    "# Create a new figure for 3D plotting\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "base_colors = ['red', 'green', 'blue']\n",
    "\n",
    "for i, author in enumerate(np.unique(y_train)):\n",
    "    idx = np.where(y_train == author)[0]  # Ensuring idx is an array of indices\n",
    "    ax.scatter(X_train_scaled[idx, 0], X_train_scaled[idx, 1], X_train_scaled[idx, 2],\n",
    "               label=f'Author {author}', s=size_norm[idx], color=base_colors[i % len(base_colors)])\n",
    "\n",
    "ax.set_xlabel('Punctuation Count')\n",
    "ax.set_ylabel('Capital Letter Count')\n",
    "ax.set_zlabel('Four Letter Word Count')\n",
    "plt.title('Feature Comparison by Author')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d9aaf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9b9db2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311cf440",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
